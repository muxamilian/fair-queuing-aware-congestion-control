\documentclass[conference]{IEEEtran}
% \IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{subfig}
\usepackage{tabularx}
\usepackage{dblfloatfix}
\usepackage{xurl}

\renewcommand{\thefootnote}{\arabic{footnote}}
\renewcommand{\thempfootnote}{\arabic{mpfootnote}}
\begin{document}

\title{Fair Queuing Aware Congestion Control}

\author{\IEEEauthorblockN{Maximilian Bachl}
\IEEEauthorblockA{{\scriptsize \url{https://github.com/muxamilian/fair-queuing-aware-congestion-control}}}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
}

\maketitle
\thispagestyle{plain}
\pagestyle{plain}

\begin{abstract}
Fair queuing is becoming increasingly prevalent in the internet and has been shown to improve performance in many circumstances. 
Performance could be improved even more if endpoints could detect the presence of fair queuing on a certain path and adjust their congestion control accordingly. 
If fair queuing is detected, the congestion control would not have to take cross traffic into account, which allows for more flexibility. 
In this paper, we develop the first algorithm that continuously checks if fair queuing is present on a path, with very high accuracy. 
When fair queuing is detected, a different congestion control can be chosen, which can result in reduced latency. In case fair queuing is detected, each flow can specify how much queuing delay it allows, meaning that each flow can choose its own tradeoff between throughput and latency. 
\end{abstract}

% \begin{IEEEkeywords}
% component, formatting, style, styling, insert
% \end{IEEEkeywords}

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{{"../sample/plots/concept_fq"}.pdf}
\caption{In case there is \textbf{fair queuing}, when the bottleneck link is congested, subflow 1, which sends more, sees increasing queuing delay. Subflow 2 doesn't see increased queuing delay as it sends less than its fair share and thus its packets don't have to wait at the bottleneck link.}
\label{fig:illustration_fq}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{{"../sample/plots/concept_fcfs"}.pdf}
\caption{In case there is \textbf{no fair queuing}, when the bottleneck link is congested, a queue builds up. Since both subflows share the same queue, they both see increased queuing delay.}
\label{fig:illustration_no_fq}
\end{figure}    

\section{Background}

When different applications send packets over the internet, one application can send more than the other and thus unfairly take a larger share of bandwidth. Also, a large queue can form if one flow sends more than it should. 
This can result in unfairness and bad user experience. Several different approaches have been proposed to address this \cite{brown_future_2020,ware_beyond_2019}: One is to make sure every network flow is ``well-behaved'' (also known as TCP friendly). 
Another one is to enforce fairness at switches and routers, called ``fair queuing'' or ``flow queuing'' \cite{nagle_packet_1985}. 

While fair queuing was proposed decades ago, it only gained popularity in the last couple of years because of implementations in the Linux kernel \cite{dumazet_pkt_sched_2013,hoeiland-joergensen_flow_2018} and also in Apple's macOS. 
Applications can benefit from increasing deployment of fair queuing: It makes sure that not the most aggressive one wins. 
It would be even better if applications could know if the connection they're sending on is managed by fair queuing. 
Then they could be sure that they can use their preferred congestion control mechanism, while not bothering or being bothered by other network flows. 
We proposed the first such approach in previous work \cite{bachl_detecting_2021} but our previous approach had some shortcomings upon which we improve in this paper. 

In our previous work we proposed a technique which determines the presence of fair queuing at flow startup, which works as follows: 
If fair queuing is successfully detected at flow startup, a congestion control was used which aimed to keep queuing delay low (delay-based congestion control). 
While this delay-based congestion control achieved high throughput and low delay, similar to the Vegas congestion control algorithm \cite{brakmo_tcp_1995}, 
it was vulnerable to be outcompeted by other network flows sending more aggressively, such as \cite{cardwell_bbr_2016,dong_pcc_2015,ha_cubic_2008}.
This means that our delay-based congestion control performed well but only when it wouldn't have to compete with other flows. Thus it is only used if fair queuing is detected. 
If the absence of fair queuing is detected, our previous approach uses a more aggressive congestion control (specifically PCC \cite{dong_pcc_2015}), which can compete better with other, more aggressive network flows,
but doesn't keep delay as low as our delay-based congestion control. 

While our previous approach had high detection accuracy (98\%), it also had some limitations:
\begin{itemize}
    \item It would \textbf{only} detect fair queuing only at \textbf{flow startup}. 
    But if the bottleneck link changes during a flow, it could be that the previous bottleneck had fair queuing while the new one doesn't. This wouldn't have been detected. 
    \item It would detect fair queuing only after filling the queue at the bottleneck completely, \textbf{causing packet loss}. 
\end{itemize}

We would thus like to have an approach which 
\begin{itemize}
    \item \textbf{continuously checks} for the presence or absence of fair queuing, not only at flow startup. 
    \item \textbf{doesn't cause packet loss} while trying to determine if there is fair queuing or not. 
    \item can be \textbf{transparently used} on top of any congestion control algorithm. 
    \item lets each application \textbf{choose how much delay} it allows in case fair queuing is detected. Allowing a higher delay can result in higher throughput. 
\end{itemize} 

\section{Concept}

Our new approach presented here is a mechanism which performs measurements to determine if there is fair queuing. Our mechanism can be added on top of any existing congestion control algorithm. 
The approach of performing measurements in congestion control became popular in the last couple of years and was already followed by \cite{cardwell_bbr_2016,dong_pcc_2015,goyal_elasticity_2020,hayes_online_2020}.

The core concept of our approach, which we call \textbf{\textit{Tonopah}}, is that a network flow is separated into \textbf{two subflows}. 
One flow constantly sends more data (\textbf{dominant subflow}), while the other one sends less (\textbf{non-dominant subflow}). 
When the total sending rate reaches the link capacity and fair queuing is present, 
\textbf{fair queuing} is going to \textbf{limit} the throughput of the \textbf{dominant subflow} (\autoref{fig:illustration_fq}). 
Since its throughput is limited, a queue is going to build up and delay is going to increase. If the delay only increases for the dominant subflow but not for the non-dominant one, this means that there's fair queuing. 

On the other side, if there is no fair queuing, when the sending rate exceeds the capacity of the bottleneck link, a queuing is going to build up and queuing delay is going to rise for \textbf{both} the dominant and the non-dominant subflow (\autoref{fig:illustration_no_fq}). 
In this case, Tonopah doesn't behave different from a network flow which doesn't have fair queuing detection. 
This can be a potential advantage which a Tonopah-enabled congestion control can have over other congestion control algorithms such as BBR, which has been shown to be unfair to other flows sometimes \cite{ware_modeling_2019,hock_experimental_2017}.

Thanks to the use of Multipath QUIC \cite{liu_multipath_2022} the application does not see any difference because all data are recombined to one flow by Multipath QUIC. 

An interesting aspect of fair queuing detection is that the presence of fair queuing can be interpreted as a \textit{congestion signal}: 
Only if there is congestion, fair queuing can be detected. If the link is underutilized, flows don't ``fight'' for bandwidth and thus fairness doesn't need to be enforced at the bottleneck link. 
Thus, in this paper, we define a new congestion signal: The presence of fair queuing. Other already known congestion signals are, for example: 
packet loss, queue length, change of queue length, Explicit Congestion Notification \cite{mathis_relentless_2009,hayes_revisiting_2011}. 

\subsection{Details}

The dominant subflow receives $\frac{2}{3}$ of the total allowed sending rate, while the non-dominant one receives the remaining $\frac{1}{3}$. 

After each round-trip time, Tonopah checks if the queuing delay of dominant subflow was on average larger than the one of the non-dominant subflow. As the threshold we use 5\,ms: If the average queuing delay in the last round-trip time was 5\,ms higher for the dominant subflow compared to the non-dominant one, fair queuing is detected. However, it is worth noting that each application can choose this threshold individually. For example, a cloud gaming application might choose a threshold of 5\,ms and thus prioritize low delay over high throughput. However, a video chat application might want to emphasize high bandwidth more and thus choose a threshold of 20\,ms to get maximum throughput. 

Both subflows are controlled by the same congestion window. The differences in sending rates are achieved by using pacing. 
The basis of the implementation of Tonopah is \textit{Picoquic}\footnote{\url{https://github.com/private-octopus/picoquic/tree/master/picoquic}}. 
Picoquic includes support for Multipath QUIC and pacing, which we need for the implementation of Tonopah. Also, it implements congestion control algorithms. 
We base Tonopah on the NewReno implementation of Picquic. When fair queuing is detected, the congestion window is reduced by $\frac{1}{8}$. 
This worked well for our proof-of-concept implementation but one could also specify any other behavior in case fair queuing is detected or even do nothing at all. 

\section{Evaluation}

Tonopah was implemented on top of Picoquic in C. It was added on top of the NewReno congestion control algorithm but it could also be added on top of other 
congestion control algorithms as well. By basing our implementation on Picoquic, Tonopah also supports Explicit Congestion Notification. 

We evaluated Tonopah on an Apple MacBook Air from 2011 with an Intel Core i5-2557M CPU at 1.7\,GHz. 
By choosing old hardware for experimentation, we can show that our algorithm doesn't use excessive compute and can also run well on weak hardware. 

We emulate the network using \textit{mininet} and use Linux's \textit{pfifo} queuing discipline in the case there is no fair queuing and \textit{fq} for fair queuing. 
For evaluation we choose a simple network in which the sender is connected to the receiver via a switch. 

All the source code and evaluation code of this paper is freely available (refer to the very top of this paper). 

In the following evaluations we always run each network flow for 90\,s. Tonopah measures whether there is fair queuing or not continuously. 
If in the evaluation scenario there is fair queuing and Tonopah detects fair queuing correctly for 80\,s and fails to detect it for 10\,s, we 
state the accuracy as $\frac{80}{90} = 89\%$. 
We evaluate the performance of Tonopah over a range of delays from 10 to 100\,ms and link capacity from 10 to 100\,Mbit/s in a grid-like fashion. 
For each combination of delay and link capacity, multiple experiment runs are performed and the median, the quartiles or the average of the results are shown. 
As the buffer size at the bottleneck we set the bandwidth delay product but at least 100 packets. 

\subsection{Detection accuracy}

\begin{table}
\begin{tabularx}{\columnwidth}{| l | X | X | X |}
\hline
& 10 Mbit/s & 50 Mbit/s & 100 Mbit/s \\ \hline
10ms & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\%\\ \hline
50ms & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% \\ \hline
100ms & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% \\ \hline
\end{tabularx}
\caption{Detection accuracy in case there's \textbf{no fair queuing}.  The overall median accuracy (true negative rate) is 100\%, the first quartile is 100\% and the third quartile is 100\%.}
\label{table:no_fq}
\end{table}

\begin{table}
\begin{tabularx}{\columnwidth}{| l | X | X | X |}
\hline
& 10 Mbit/s & 50 Mbit/s & 100 Mbit/s \\ \hline
10ms & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 99\% \newline 3rd quart.: 100\%\\ \hline
50ms & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% \\ \hline
100ms & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% \\ \hline
\end{tabularx}
\caption{Detection accuracy in case there is \textbf{fair queuing}.  The overall median accuracy (true positive rate) is 100\%, the first quartile is 100\% and the third quartile is 100\%.}
\label{table:fq}
\end{table}    

\autoref{table:no_fq} shows that if there is no fair queuing, Tonopah detects this correctly virtually always (true negative rate $\geq$99\%). 
If there is fair queuing (\autoref{table:fq}), Tonopah detects this correctly very often but not in every case (true positive rate in the high 90s). 

\subsection{Throughput and Queuing Delay of Tonopah}

\begin{table}
\begin{minipage}{\columnwidth}
\centering
\begin{tabular}{| r | r | r |}
\hline
& Link utilization & Queuing delay \\ \hline
NewReno & 95.6\% & 33.6 ms \\ \hline
Tonopah & 94.3\% & 10.5 ms \\ \hline
% p-value\protect\footnote{Using Welch's t-test} & 0.86 & $1.9 \times 10^{-6}$ \\ \hline
% $\frac{\text{NewReno}}{\text{Tonopah}}$ & 103\% & 321\% \\ \hline
\end{tabular}
\caption{NewReno's link utilization is 103\% Tonopah's. 
But the \textbf{queuing delay} of NewReno is \textbf{321\%} Tonopah's. 
This means that Tonopah can deliver the same throughput while causing a lot less queuing delay. 
The differences in \textbf{queuing delay} are \textbf{highly significant} (p-value\protect\footnote{Using Welch's t-test} $< 10^{-5}$). 
The experiment setup is the same as in \autoref{table:fq}. }
\label{table:throughput_and_delay}
\end{minipage}
\end{table}    

While the goal of this paper is to implement a mechanism to detect fair queuing in an online fashion, 
we also want to show that fair queuing detection can be used to lower the queuing delay drastically while not impairing throughput. 
\autoref{table:throughput_and_delay} shows that Tonopah can lower queuing delay by more than two thirds while keeping throughput unchanged, when it is implemented on top on NewReno. 
However, we want to emphasize that Tonopah is not a congestion control algorithm but just a mechanism to measure fair queuing which can be used with any congestion control algorithm. 

\subsection{Cross-traffic}

\begin{table}
\begin{tabularx}{\columnwidth}{| l | X | X | X |}
\hline
& 10 Mbit/s & 50 Mbit/s & 100 Mbit/s \\ \hline
10ms & Median: 99\% \newline 1st quart.: 98\% \newline 3rd quart.: 99\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\%\\ \hline
50ms & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% \\ \hline
100ms & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 98\% \newline 3rd quart.: 100\% \\ \hline
\end{tabularx}
\caption{Detection accuracy in case there's \textbf{no fair queuing} under the presence of \textbf{cross-traffic}. The overall median accuracy (true negative rate) is 100\%, the first quartile is 100\% and the third quartile is 100\%.}
\label{table:no_fq_crosstraffic}
\end{table}

\begin{table}
\begin{tabularx}{\columnwidth}{| l | X | X | X |}
\hline
& 10 Mbit/s & 50 Mbit/s & 100 Mbit/s \\ \hline
10ms & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\%\\ \hline
50ms & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% \\ \hline
100ms & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% \\ \hline
\end{tabularx}
\caption{Detection accuracy in case there is \textbf{fair queuing} under the presence of \textbf{cross-traffic}.  The overall median accuracy (true positive rate) is 100\%, the first quartile is 100\% and the third quartile is 100\%.}
\label{table:fq_crosstraffic}
\end{table}        

To evaluate Tonopah's performance under crosstraffic, we use iperf with the NewReno congestion control. We start iperf 4\,s before Tonopah so that it has time to saturate the link. 
\autoref{table:no_fq_crosstraffic} and \autoref{table:fq_crosstraffic} show that also in the presence of cross-traffic Tonpah's detection accuracy is virtually unchanged. 

\subsection{Other variants of fair queuing: fq\_codel}

\begin{table}
\begin{tabularx}{\columnwidth}{| l | X | X | X |}
\hline
& 10 Mbit/s & 50 Mbit/s & 100 Mbit/s \\ \hline
10ms & Median: 85\% \newline 1st quart.: 84\% \newline 3rd quart.: 85\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\%\\ \hline
50ms & Median: 91\% \newline 1st quart.: 88\% \newline 3rd quart.: 92\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 98\% \newline 1st quart.: 97\% \newline 3rd quart.: 99\% \\ \hline
100ms & Median: 9\% \newline 1st quart.: 7\% \newline 3rd quart.: 13\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 64\% \newline 1st quart.: 0\% \newline 3rd quart.: 75\% \\ \hline
\end{tabularx}
\caption{Detection accuracy in case there is the \textbf{\textit{fq\_codel}} variant of \textbf{fair queuing} with a target delay of 10\,ms. The overall median accuracy (true positive rate) is 95\%, the first quartile is 83\% and the third quartile is 100\%.}
\label{table:fq_codel}
\end{table}        

We also evaluated the performance of our fair queuing detection on a bottleneck managed by fq\_codel \cite{hoeiland-joergensen_flow_2018}. 
We chose a default target queuing delay of 10\,ms following Apple's implementation\footnote{
    \url{https://github.com/apple/darwin-xnu/blob/2ff845c2e033bd0ff64b5b6aa6063a1f8f65aa32/bsd/net/if.h\#L262}
} because we argue that Apple probably spent a considerable amount of time fine-tuning their implementation and 
came to the conclusion that 10\,ms work best as the default target delay. 
Also, Apple's implementation of fq\_codel seems to be the most widespread worldwide\footnote{\url{https://blog.cerowrt.org/post/state_of_fq_codel/}} with currently two billion Apple devices presumably supporting Apple's fq\_codel. 

\autoref{table:fq_codel} shows that Tonopah doesn't detect fair queuing as well when there's fq\_codel compared to when there is fq. 
However, median detection accuracy (true positive rate) is still reasonably high at 95\%.

\subsubsection{fq\_codel with a target delay of 5\,ms}

\begin{table}
\begin{tabularx}{\columnwidth}{| l | X | X | X |}
\hline
& 10 Mbit/s & 50 Mbit/s & 100 Mbit/s \\ \hline
10ms & Median: 39\% \newline 1st quart.: 37\% \newline 3rd quart.: 80\% & Median: 100\% \newline 1st quart.: 99\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\%\\ \hline
50ms & Median: 36\% \newline 1st quart.: 36\% \newline 3rd quart.: 36\% & Median: 100\% \newline 1st quart.: 99\% \newline 3rd quart.: 100\% & Median: 99\% \newline 1st quart.: 98\% \newline 3rd quart.: 99\% \\ \hline
100ms & Median: 16\% \newline 1st quart.: 14\% \newline 3rd quart.: 17\% & Median: 99\% \newline 1st quart.: 98\% \newline 3rd quart.: 100\% & Median: 97\% \newline 1st quart.: 64\% \newline 3rd quart.: 99\% \\ \hline
\end{tabularx}
\caption{Detection accuracy in case there is the \textbf{\textit{fq\_codel}} variant of \textbf{fair queuing} with a target delay of 5\,ms. The overall median accuracy (true positive rate) is 98\%, the first quartile is 37\% and the third quartile is 100\%.}
\label{table:fq_codel_linux}
\end{table}            

Although Apple's fq\_codel implementation with its 10\,ms default target delay seems to be the most widespread, fq\_codel is also the default queuing manager on multiple Linux distributions. 
On Linux, fq\_codel has a default target delay of 5\,ms, half of Apple's value of 10. 

Since the target delay of 5\,ms is quite small, we had to slightly modify Tonopah: We fixed the interval of Tonopah to 50\,ms. 
As a reminder, the interval is the time duration after which the dominant subflow becomes the non-dominant one and vice versa. 
As a reminder, for the previous experiments, the interval was the current round trip time but at least 50\,ms and no more than 100\,ms. 

\autoref{table:fq_codel_linux} shows the performance of Tonopah with Linux's implementation of fq\_codel. Median detection accuracy (true positive rate) is 98\%. 
A problem is that accuracy is pretty low for small bandwidths ($\leq$10\,Mbit/s) but otherwise fair queuing can be recognized quite accurately. 

Since we changed how Tonopah works to make it work better with Linux's implementation of fq\_codel by setting the interval to 50\,ms, we also have to make sure that this modification didn't lower the true negative rate (accuracy in case there is no fair queuing). 
Our experiments show that with the constant 50\,ms interval Tonopah even works slightly better, giving a true negative rate of 100\%, meaning it can always detect the absence of fair queuing correctly.  

\section{Discussion}

We showed that continuous fair queuing detection is feasible and can achieve a high detection accuracy. 
We also showed that it can work with alternative versions of fair queuing such as fq\_codel although experiments showed that accuracy depends on the configuration of the queue manager, 
the link capacity, base delay etc. Further experiments could be done to finetune Tonopah and also experiment with other queue managers such as \textit{cake} \cite{hoiland-jorgensen_piece_2018}, \textit{PIE} \cite{pan_pie_2013}, \textit{cocoa} \cite{bachl_cocoa_2019} or \textit{LFQ} \cite{bachl_lfq_2020}. 

Another potentially fruitful line of research would be to integrate Tonopah into other congestion control algorithms such as BBR. 

\section*{Acknowledgements}

We thank Sebastian Moeller, who suggested to also include experiments with fq\_codel with a target value of 5\,ms, which is the default on Linux. 

\bibliographystyle{ieeetr}
\bibliography{fair-queuing-aware-congestion-control}

\end{document}
