\documentclass[conference]{IEEEtran}
% \IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{subfig}
\usepackage{tabularx}
\usepackage{dblfloatfix}
\renewcommand{\thefootnote}{\arabic{footnote}}
\renewcommand{\thempfootnote}{\arabic{mpfootnote}}
\begin{document}

\title{Fair Queuing Aware Congestion Control}

\author{\IEEEauthorblockN{Maximilian Bachl}
\IEEEauthorblockA{{\scriptsize \url{https://github.com/muxamilian/fair-queuing-aware-congestion-control}}}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
}

\maketitle
\thispagestyle{plain}
\pagestyle{plain}

\begin{abstract}
Fair queuing is becoming increasingly prevalent in the internet and has been shown to improve performance in many circumstances. 
Performance could be improved even more if endpoints could detect the presence of fair queuing on a certain path and adjust their congestion control accordingly. 
If fair queuing is detected, the congestion control would not have to take cross traffic into account, which allows for more flexibility. 
In this paper, we develop the first algorithm that continuously checks if fair queuing is present on a path. 
When fair queuing is detected, a different congestion control can be chosen, which can result in reduced latency. 
Unlike an algorithm proposed in a previous paper of us, the approach presented here does not only detect the presence of fair queuing once at flow startup but it does so continuously. 
% We show that by adding fair queuing detection to an existing congestion control algorithm, queuing delay can be reduced by more than two thirds. 
\end{abstract}

% \begin{IEEEkeywords}
% component, formatting, style, styling, insert
% \end{IEEEkeywords}

\begin{figure}[h]
\centering
\subfloat[\footnotesize \textbf{Sending} rate at the sender (fair queuing)]{\includegraphics[width=\columnwidth]{{"figures/bw_server_fq"}.pdf}
\label{fig:sender}}\\
\subfloat[\footnotesize \textbf{Receiving} rate at the receiver (fair queuing)]{\includegraphics[width=\columnwidth]{{"figures/bw_client_fq"}.pdf}
\label{fig:receiver}}
\caption{Figure \ref{fig:sender} shows the sending rate, \ref{fig:receiver} the receiving rate in case there's \textbf{fair queuing}. 
Around seconds 82 and 83.5, the total sending rate is exceeding the link capacity -- 50 Mbit/s -- and fair queuing starts to limit the throughput of the subflow that is sending more. 
Thus, in the lower figure, the dominant subflow and the non-dominant subflow achieve approx.~the same receiving rate even though the dominant subflow sends more (dominant subflow). This is the effect of fair queuing.}
\label{fig:illustration_fq}
\end{figure}

\begin{figure}[h]
\centering
\subfloat[\footnotesize \textbf{Sending} rate at the sender (no fair queuing)]{\includegraphics[width=\columnwidth]{{"figures/bw_server_pfifo"}.pdf}
\label{fig:sender_pfifo}}\\
\subfloat[\footnotesize \textbf{Receiving} rate at the receiver (no fair queuing)]{\includegraphics[width=\columnwidth]{{"figures/bw_client_pfifo"}.pdf}
\label{fig:receiver_pfifo}}
\caption{Figure \ref{fig:sender_pfifo} shows the sending rate, \ref{fig:receiver_pfifo} the receiving rate in case there's \textbf{no fair queuing}. 
Even though the sender sends too much and a queue builds up, still the flow that sends more data also has a higher receiving rate. 
This is in contrast to \autoref{fig:illustration_fq}, where both flows have the same receiving rate once there is congestion at the bottleneck, thanks to fair queuing. }
\label{fig:illustration_no_fq}
\end{figure}    

\section{Background}

When different applications send packets over the internet, one application can send more than the other and thus unfairly take a larger share of bandwidth. 
This can result in unfairness and bad user experience. Several different approaches have been proposed to address this \cite{brown_future_2020,ware_beyond_2019}: One is to make sure every network flow is ``well-behaved'' (also known as TCP friendly). 
Another one is to enforce fairness at switches and routers, called ``fair queuing'' or ``flow queuing'' \cite{nagle_packet_1985}. 

While fair queuing was proposed decades ago, it only gained popularity in the last couple of years because of implementations in the Linux kernel \cite{dumazet_pkt_sched_2013,hoeiland-joergensen_flow_2018} and also in Apple's macOS. 
Applications can benefit from increasing deployment of fair queuing: It makes sure that not the most aggressive one wins. 
It would be even better if applications could know if the connection they're sending on is managed by fair queuing. 
Then they could be sure that they can use their preferred congestion control mechanism, while not bothering or being bothered by other network flows. 
We proposed the first such approach in previous work \cite{bachl_detecting_2021} but our previous approach had some shortcomings upon which we improve in this paper. 

In our previous work we proposed a technique which determines the presence of fair queuing at flow startup, which worked as follows: 
If fair queuing is successfully detected at flow startup, a congestion control was used, which aimed to keep queuing delay low (delay-based congestion control). 
While this delay-based congestion control achieved high throughput and low delay, it was vulnerable to be outcompeted by other network flows sending more aggressively, such as \cite{cardwell_bbr_2016,dong_pcc_2015,ha_cubic_2008}, 
similar to the Vegas congestion control algorithm \cite{brakmo_tcp_1995}.
This means that our delay-based congestion control performed well but only when it wouldn't have to compete with other flows. Thus it is only used if fair queuing is detected. 
If the absence of fair queuing is detected, our previous approach uses a more aggressive congestion control (specifically PCC \cite{dong_pcc_2015}), which can compete better with other network flows,
but doesn't keep delay as low as our delay-based congestion control. 

While our previous approach had high detection accuracy (98\%), it also had the following limitations:
\begin{itemize}
    \item It would \textbf{only} detect fair queuing at \textbf{flow startup}. 
    But if the bottleneck link changes during a flow, it could be that the previous bottleneck had fair queuing while the new one doesn't. This wouldn't have been detected. 
    \item It would detect fair queuing only after filling the queue at the bottleneck completely, \textbf{causing packet loss}. 
\end{itemize}

We would thus like to have an approach which 
\begin{itemize}
    \item \textbf{continuously checks} for the presence or absence of fair queuing, not only at flow startup. 
    \item \textbf{doesn't cause packet loss} while trying to determine if there is fair queuing or not. 
    \item can be \textbf{transparently used} on top of any congestion control algorithm. 
\end{itemize} 

\section{Concept}

Our new approach presented here is a mechanism which performs measurements to determine if there is fair queuing. Our mechanism can be added on top of any existing congestion control algorithm. 
The approach of performing measurements in congestion control became popular in the last couple of years and was already followed by \cite{cardwell_bbr_2016,dong_pcc_2015,goyal_elasticity_2020,hayes_online_2020}.

The core concept of our approach, which we call \textbf{\textit{Tonopah}}, is that a network flow is separated into \textbf{two subflows}. 
\textbf{Alternatingly}, one flow sends more (\textbf{dominant subflow}), while the other one sends less (\textbf{non-dominant subflow}). 
After a certain time interval the dominant subflow becomes the non-dominant one and vice versa. 
When the total sending rate reaches the link capacity and fair queuing is present, 
\textbf{fair queuing} is going to \textbf{limit} the throughput of the \textbf{dominant subflow} (\autoref{fig:illustration_fq}). 
This can be measured by observing the \textit{delivery rate}
\footnote{The delivery rate (a concept popularized by BBR \cite{cardwell_bbr_2016}) can be measured by observing the rate at which acknowledgement packets are sent back to the sender by the receiver.
If the sender sends more than the link capacity, the delivery rate is never going to exceed the link capacity since the receiver cannot receive packets faster than the link capacity allows. 
}
of both subflows. If there is no fair queuing, the subflow which sends more also achieves a higher throughput (\autoref{fig:illustration_no_fq}), which can be considered unfair. 
In this case, Tonopah doesn't behave different from a network flow which doesn't have fair queuing detection. 

By alternating which subflow is dominant and which one is non-dominant, we prevent the build-up of a standing queue at the bottleneck: 
When the dominant subflow builds up a queue at the bottleneck link, it is going to be the non-dominant subflow in the next time interval and then it is going to send less and the queue is going to be reduced. 

The congestion window of the flow applies to both subflows together. This means that while always the dominant subflow sends more than the non-dominant one, in sum they always send the same amount of data, as allowed by the congestion window.

Thanks to the use of Multipath QUIC \cite{liu_multipath_2022} the application does not see any difference because all data are recombined to one flow by Multipath QUIC. 

An interesting aspect of fair queuing detection is that the presence of fair queuing can be interpreted as a \textit{congestion signal}: 
Only if there is congestion, fair queuing can be detected. If the link is underutilized, flows don't ``fight'' for bandwidh and thus fairness doesn't need to be enforced at the bottleneck link. 
Thus, in this paper, we define a new congestion signal: The presence of fair queuing. Other already known congestion signals are, for example: 
packet loss, queue length, change of queue length, Explicit Congestion Notification \cite{mathis_relentless_2009,hutchison_revisiting_2011}. 

\subsection{Details}

The dominant subflow receives $\frac{6}{10}$ of the total allowed sending rate, while the non-dominant one receives $\frac{4}{10}$. 
The dominant subflow and the non-dominant subflow alternate every $\max{(50, \min{(100, \textit{rtt})})}$ milliseconds. 
This time period is also called an \textbf{\textit{interval}}. To determine the presence of fair queuing, the data of the last four intervals are evaluated. 
We look at more than one interval because this reduces the effect of measurement noise. We found four intervals to be a suitable number. 

We determine that fair queuing is present if the receiving rate was more fair than not fair. 
It would be completely fair if both flows get $\frac{1}{2}$ of the total throughput. It would be completely unfair if the dominant subflow got $\frac{6}{10}$ and the non-dominant one $\frac{4}{10}$. 
If the dominant one got a fraction of 0.549 of the total throughput, it would be just a little bit more fair than unfair. In this case we would say that there is fair queuing. 
If, on the other side, the dominant subflow got a fraction of 0.551 of the total throughput, we would say that there is no fair queuing. 

Another safeguard against measurement noise is that we only take a decision regarding fair queuing if the dominant flow actually really sent more data than the non-dominant one. 
Specifically, if the dominant flow sends less than 57.5\%, we don't take a decision because it means that probably we didn't have enough data to send. 
This case only occurs when the underlying application has no data to send or when the sending rate is very low ($<$ 10\,Mbit/s). 

Both subflows are controlled by the same congestion window. The differences in sending rates are implemented by using pacing. 
The basis of the implementation of Tonopah is \textit{Picoquic}\footnote{\url{https://github.com/private-octopus/picoquic/tree/master/picoquic}}. 
Picoquic includes support for Multipath QUIC and pacing, which we need for the implementation of Tonopah. Also, it implements congestion control algorithms. 
We base Tonopah on the NewReno implementation of Picquic. When fair queuing is detected, the congestion window is reduced by $\frac{1}{8}$. 
This worked well for our proof-of-concept implementation but one could also specify any other behavior in case fair queuing is detected or even do nothing at all. 

\section{Evaluation}

Tonopah was implemented on top of Picoquic in C. It was added on top of the NewReno congestion control algorithm but it could easily be added on top of other 
congestion control algorithms as well. By basing our implementation on Picoquic, Tonopah also supports Explicit Congestion Notification. 

We evaluated Tonopah on an Apple MacBook Air from 2011 with an Intel Core i5-2557M CPU at 1.70GHz. 
By choosing old hardware for experimentation, we can show that our algorithm doesn't use excessive compute and can also run well on weak hardware. 

We emulate the network using mininet and use Linux's \textit{pfifo} queuing discipline in the case there is no fair queuing in \textit{fq} for fair queuing. 
For evaluation we choose a network in which the sender is connected to the receiver via a switch (dumbbell topology). 

All the source code and evaluation code of this paper are freely available (refer to the very top of this paper). 

In the following evaluations we always run each network flow for 90\,s. Tonopah measures whether there is fair queuing or not continuously. 
If in the evaluation scenario there is fair queuing and Tonopah detects fair queuing for 80\,s and fails to detect it for 10\,s, we 
state the accuracy as $\frac{80}{90} = 89\%$. 
We evaluate the performance of Tonopah over a range of delays from 10 to 100\,ms and link capacity from 10 to 100\,Mbit/s. 
\begin{table}
\begin{tabularx}{\columnwidth}{| l | X | X | X |}
\hline
& 10 Mbit/s & 50 Mbit/s & 100 Mbit/s \\ \hline
10ms & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\%\\ \hline
50ms & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% \\ \hline
100ms & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% \\ \hline
\end{tabularx}
\caption{Detection accuracy in case there's \textbf{no fair queuing}.  The overall median accuracy is 100\%, the first quartile is 100\% and the third quartile is 100\%.}
\label{table:no_fq}
\end{table}

\begin{table}
\begin{tabularx}{\columnwidth}{| l | X | X | X |}
\hline
& 10 Mbit/s & 50 Mbit/s & 100 Mbit/s \\ \hline
10ms & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 99\% \newline 3rd quart.: 100\%\\ \hline
50ms & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% \\ \hline
100ms & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% \\ \hline
\end{tabularx}
\caption{Detection accuracy in case there is \textbf{fair queuing}.  The overall median accuracy is 100\%, the first quartile is 100\% and the third quartile is 100\%.}
\label{table:fq}
\end{table}    

\subsection{Detection accuracy}

\autoref{table:no_fq} shows that if there is no fair queuing, Tonopah detects this correctly virtually always. 
If there is fair queuing (\autoref{table:fq}), Tonopah detects this correctly very often but not in every case. 

\subsection{Throughput and Queuing Delay of Tonopah}

\begin{table}
\begin{minipage}{\columnwidth}
\centering
\begin{tabular}{| r | r | r |}
\hline
& Link utilization & Queuing delay \\ \hline
NewReno & 95.6\% & 10.5 ms \\ \hline
Tonopah & 94.3\% & 33.6 ms \\ \hline
% p-value\protect\footnote{Using Welch's t-test} & 0.86 & $1.9 \times 10^{-6}$ \\ \hline
% $\frac{\text{NewReno}}{\text{Tonopah}}$ & 103\% & 321\% \\ \hline
\end{tabular}
\caption{NewReno's link utilization is 103\% Tonopah's. 
But the \textbf{queuing delay} of NewReno is \textbf{321\%} Tonopah's. 
This means that Tonopah can deliver the same throughput while cause a lot less queuing delay. 
The differences in \textbf{queuing delay} are \textbf{highly significant} (p-value\protect\footnote{Using Welch's t-test} $< 10^{-5}$). 
The experiment setup is the same as in \autoref{table:fq}. }
\label{table:throughput_and_delay}
\end{minipage}
\end{table}    

While the goal of this paper is to implement a mechanism to detect fair queuing in an online fashion, 
we also want to show that fair queuing detection can be used to lower the queuing delay drastically while not impairing throughput. 
\autoref{table:throughput_and_delay} shows that Tonopah can lower queuing delay by more than two thirds while keeping throughput unchanged, when it is implemented on top on NewReno. 
However, we want to emphasize that Tonopah is not a congestion control algorithm but just a mechanism to measure fair queuing which can be used with any congestion control algorithm. 

\subsection{Cross-traffic}

\begin{table}
\begin{tabularx}{\columnwidth}{| l | X | X | X |}
\hline
& 10 Mbit/s & 50 Mbit/s & 100 Mbit/s \\ \hline
10ms & Median: 100\% \newline 1st quart.: 99\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 98\% \newline 3rd quart.: 99\%\\ \hline
50ms & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% \\ \hline
100ms & Median: 100\% \newline 1st quart.: 98\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% \\ \hline
\end{tabularx}
\caption{Detection accuracy in case there's \textbf{no fair queuing} under the presence of \textbf{cross-traffic}. The overall median accuracy is 100\%, the first quartile is 100\% and the third quartile is 100\%.}
\label{table:no_fq_crosstraffic}
\end{table}

\begin{table}
\begin{tabularx}{\columnwidth}{| l | X | X | X |}
\hline
& 10 Mbit/s & 50 Mbit/s & 100 Mbit/s \\ \hline
10ms & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\%\\ \hline
50ms & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% \\ \hline
100ms & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% \\ \hline
\end{tabularx}
\caption{Detection accuracy in case there is \textbf{fair queuing} under the presence of \textbf{cross-traffic}.  The overall median accuracy is 100\%, the first quartile is 100\% and the third quartile is 100\%.}
\label{table:fq_crosstraffic}
\end{table}        

To evaluate Tonopah's performance under crosstraffic, we use iperf with the NewReno congestion control. We start iperf 4\,s before Tonopah so that it has time to saturate the link. 
\autoref{table:no_fq_crosstraffic} and \autoref{table:fq_crosstraffic} show that also in the presence of cross-traffic Tonpah's detection accuracy is virtually unchanged. 

\subsection{Other variants of fair queuing}

\begin{table}
\begin{tabularx}{\columnwidth}{| l | X | X | X |}
\hline
& 10 Mbit/s & 50 Mbit/s & 100 Mbit/s \\ \hline
10ms & Median: 85\% \newline 1st quart.: 84\% \newline 3rd quart.: 85\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\%\\ \hline
50ms & Median: 91\% \newline 1st quart.: 88\% \newline 3rd quart.: 92\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 98\% \newline 1st quart.: 97\% \newline 3rd quart.: 99\% \\ \hline
100ms & Median: 9\% \newline 1st quart.: 7\% \newline 3rd quart.: 13\% & Median: 100\% \newline 1st quart.: 100\% \newline 3rd quart.: 100\% & Median: 64\% \newline 1st quart.: 0\% \newline 3rd quart.: 75\% \\ \hline
\end{tabularx}
\caption{Detection accuracy in case there is the \textbf{\textit{fq\_codel}} variant of \textbf{fair queuing}. The overall median accuracy is 95\%, the first quartile is 83\% and the third quartile is 100\%.}
\label{table:fq_codel}
\end{table}        

We also evaluated the performance of our fair queuing detection on a bottleneck managed by fq\_codel \cite{hoeiland-joergensen_flow_2018}. 
We chose a default target queuing delay of 10\,ms following Apple's implementation\footnote{
    \url{https://github.com/apple/darwin-xnu/blob/main/bsd/net/if.h\#L262}
} because we argue that Apple probably spent a considerable amount of time fine-tuning their implementation and 
came to the conclusion that 10\,ms work best as the default target delay. 

\autoref{table:fq_codel} shows that Tonopah doesn't detect fair queuing as well when there's fq\_codel compared to when there is fq. Median detection accuracy is 95\%.

\section{Discussion}

We showed that continuous fair queuing detection is feasible and can achieve a high detection accuracy. 
We also showed that it can work with alternative versions of fair queuing such as fq\_codel although accuracy depends on the configuration of the queue manager, 
the link capacity, base delay etc. Further experiments could be done to finetune Tonopah and also experiment with other queue managers such as \textit{cocoa} \cite{bachl_cocoa_2019} or \textit{cake} \cite{hoiland-jorgensen_piece_2018}. 

Another potentially fruitful line of research would be to integrate Tonopah into other congestion control algorithms such as BBR. 

\bibliographystyle{ieeetr}
\bibliography{fair-queuing-aware-congestion-control}

\end{document}
